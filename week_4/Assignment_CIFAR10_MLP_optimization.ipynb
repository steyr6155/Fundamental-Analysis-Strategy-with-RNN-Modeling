{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emh1lXeYaMVF"
      },
      "source": [
        "# CIFAR 10 with a MLP in PyTorch\n",
        "We return to our problem form last week and try to optimize our solution:\n",
        "* use TensorBoard to visualize the training\n",
        "* try different Losses\n",
        "* try different Optimizers\n",
        "* try different hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D7enuKthZuFC",
        "outputId": "09802b82-c1e1-48e7-ec3b-5ed0640e3090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m833.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "362a8fb0e089413abc3f05365575385a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2rui6YPgaWHn"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ib9AeEvawaZ"
      },
      "source": [
        "## Get CIFAR\n",
        "Use PyTorch Data Loaders (more next week) to get data batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_5YooJWsadqw"
      },
      "outputs": [],
      "source": [
        "#transform input data (image) to tensor\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "#set batch size\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "qSerMWeDa6UA",
        "outputId": "c630f194-e8f4-4af3-bd6e-b43f034b79d5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "horse cat   dog   deer \n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter) #get data from loader!\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cXvRBHIWbKdX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re0XcPj6daEj"
      },
      "source": [
        "## Assignment: design MLP to classify CIFAR10\n",
        "\n",
        "### E1 use TensorBoard to visualize the training\n",
        "\n",
        "### E2 try two different Losses\n",
        "\n",
        "### E3 try two different Optimizers\n",
        "* SGD with Momentum\n",
        "* AdaGrad or Adam\n",
        "\n",
        "### E4 implement a learning rate schedule\n",
        "* use SGD with Momentung as Optimizer\n",
        "* use a Multi-step schedule\n",
        "\n",
        "### Notes:\n",
        "* USE THE GPU! -> need to transfer the model and data to the GPU\n",
        "* MLP take 1D input - CIFAR imges are 2D -> first operator of your net needs to flatten the image\n",
        "* CIFAR is a multi class problem: use a SOFTMAX layer to output vector of class propabilities -> user argmax to get the class lable\n",
        "* Start small: use a small net with a reducet training set and a few epochs to test your setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iHcYNEmIegDu",
        "outputId": "2fe4e2a4-3a8d-47e0-ef2a-cfe825c46725",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# cifar10_mlp.py –\\xa0Rev‑3 (final bracket fix, fully syntactically valid)\\n\\n\\nfrom __future__ import annotations\\nimport argparse\\nimport random\\nimport sys\\nfrom datetime import datetime\\nfrom pathlib import Path\\n\\nimport torch\\nimport torch.backends.cudnn as cudnn\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torchvision\\nimport torchvision.transforms as T\\nfrom torch.utils.tensorboard import SummaryWriter\\n\\n\\n# -------------------------------------------------\\n# Argument parsing (swallows unknown Jupyter flags)\\n# -------------------------------------------------\\n\\ndef parse_args(argv: list[str] | None = None) -> argparse.Namespace:\\n    p = argparse.ArgumentParser(\"CIFAR‑10 MLP trainer\", add_help=True)\\n\\n    # General\\n    p.add_argument(\"--epochs\", type=int, default=20)\\n    p.add_argument(\"--batch-size\", type=int, default=128)\\n    p.add_argument(\"--subset\", type=int, default=None,\\n                   help=\"Use only N training samples for quick iteration\")\\n    p.add_argument(\"--data-root\", type=str, default=\"./data\")\\n    p.add_argument(\"--workers\", type=int, default=4)\\n    p.add_argument(\"--seed\", type=int, default=42)\\n\\n    # Model\\n    p.add_argument(\"--hidden-dims\", type=int, nargs=\"*\", default=[1024, 512, 256])\\n    p.add_argument(\"--dropout\", type=float, default=0.0)\\n\\n    # Optimizer / LR\\n    p.add_argument(\"--optimizer\", choices=[\"sgd\", \"adam\", \"adagrad\"], default=\"sgd\")\\n    p.add_argument(\"--lr\", type=float, default=0.1)\\n    p.add_argument(\"--momentum\", type=float, default=0.9)\\n    p.add_argument(\"--weight-decay\", type=float, default=1e-4)\\n    p.add_argument(\"--milestones\", type=int, nargs=\"*\", default=[60, 120, 160])\\n    p.add_argument(\"--gamma\", type=float, default=0.2)\\n\\n    # Loss\\n    p.add_argument(\"--loss\", choices=[\"cross_entropy\", \"label_smooth\", \"mse\"],\\n                   default=\"cross_entropy\")\\n    p.add_argument(\"--label-smoothing\", type=float, default=0.1,\\n                   help=\"Only used with label_smooth loss option\")\\n\\n    # Misc\\n    p.add_argument(\"--log-dir\", type=str, default=\"runs\")\\n    p.add_argument(\"--print-freq\", type=int, default=100)\\n    p.add_argument(\"--deterministic\", action=\"store_true\",\\n                   help=\"Force deterministic cuDNN (slower)\")\\n\\n    args, unknown = p.parse_known_args(argv)\\n    if unknown:\\n        print(f\"[WARNING] Ignoring unknown arguments: {unknown}\")\\n    return args\\n\\n\\n# -------------------------------------------------\\n# Model definition\\n# -------------------------------------------------\\n\\nclass MLP(nn.Module):\\n    def __init__(self, in_dim: int, num_classes: int, hidden_dims: list[int], dropout: float):\\n        super().__init__()\\n        layers: list[nn.Module] = []\\n        last_dim = in_dim\\n        for h in hidden_dims:\\n            layers.append(nn.Linear(last_dim, h))\\n            layers.append(nn.ReLU(inplace=True))\\n            if dropout > 0:\\n                layers.append(nn.Dropout(dropout))\\n            last_dim = h\\n        layers.append(nn.Linear(last_dim, num_classes))\\n        self.net = nn.Sequential(*layers)\\n\\n    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\\n        x = torch.flatten(x, 1)\\n        return self.net(x)\\n\\n\\n# -------------------------------------------------\\n# Helper utilities\\n# -------------------------------------------------\\n\\ndef set_seed(seed: int, deterministic: bool = False):\\n    random.seed(seed)\\n    torch.manual_seed(seed)\\n    torch.cuda.manual_seed_all(seed)\\n    cudnn.benchmark = not deterministic\\n    cudnn.deterministic = deterministic\\n\\n\\ndef get_dataloaders(args: argparse.Namespace):\\n    transform = T.Compose([\\n        T.ToTensor(),\\n        T.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\\n    ])\\n\\n    trainset = torchvision.datasets.CIFAR10(root=args.data_root, train=True, download=True,\\n                                           transform=transform)\\n    testset = torchvision.datasets.CIFAR10(root=args.data_root, train=False, download=True,\\n                                          transform=transform)\\n\\n    if args.subset:\\n        subset_idx = torch.randperm(len(trainset))[:args.subset]\\n        trainset = torch.utils.data.Subset(trainset, subset_idx)\\n\\n    train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\\n                                              shuffle=True, num_workers=args.workers,\\n                                              pin_memory=True)\\n    test_loader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size,\\n                                             shuffle=False, num_workers=args.workers,\\n                                             pin_memory=True)\\n    return train_loader, test_loader\\n\\n\\ndef choose_loss_fn(args: argparse.Namespace):\\n    if args.loss == \"cross_entropy\":\\n        return nn.CrossEntropyLoss()\\n    if args.loss == \"label_smooth\":\\n        return nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)\\n    if args.loss == \"mse\":\\n        return nn.MSELoss()\\n    raise ValueError(args.loss)\\n\\n\\ndef choose_optimizer(params, args: argparse.Namespace):\\n    if args.optimizer == \"sgd\":\\n        return torch.optim.SGD(params, lr=args.lr, momentum=args.momentum, nesterov=True,\\n                               weight_decay=args.weight_decay)\\n    if args.optimizer == \"adam\":\\n        return torch.optim.Adam(params, lr=args.lr, weight_decay=args.weight_decay)\\n    if args.optimizer == \"adagrad\":\\n        return torch.optim.Adagrad(params, lr=args.lr, weight_decay=args.weight_decay)\\n    raise ValueError(args.optimizer)\\n\\n\\n# -------------------------------------------------\\n# Training / Evaluation loops\\n# -------------------------------------------------\\n\\ndef train_one_epoch(model, loader, loss_fn, optimizer, device, epoch, writer, print_freq):\\n    model.train()\\n    running_loss, correct, total = 0.0, 0, 0\\n    for i, (inputs, targets) in enumerate(loader):\\n        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\\n\\n        optimizer.zero_grad(set_to_none=True)\\n        outputs = model(inputs)\\n\\n        if isinstance(loss_fn, nn.MSELoss):\\n            targets_onehot = F.one_hot(targets, num_classes=10).float()\\n            loss = loss_fn(outputs, targets_onehot)\\n        else:\\n            loss = loss_fn(outputs, targets)\\n        loss.backward()\\n        optimizer.step()\\n\\n        running_loss += loss.item() * inputs.size(0)\\n        _, preds = outputs.max(1)\\n        correct += preds.eq(targets).sum().item()\\n        total += targets.size(0)\\n\\n        if (i + 1) % print_freq == 0:\\n            print(f\"Epoch [{epoch}] Iter [{i+1}/{len(loader)}]  Loss: {loss.item():.4f}\")\\n\\n    epoch_loss = running_loss / total\\n    acc = 100.0 * correct / total\\n    writer.add_scalar(\"train/loss\", epoch_loss, epoch)\\n    writer.add_scalar(\"train/acc\", acc, epoch)\\n    return epoch_loss, acc\\n\\n\\ndef evaluate(model, loader, loss_fn, device, epoch, writer):\\n    model.eval()\\n    running_loss, correct, total = 0.0, 0, 0\\n    with torch.no_grad():\\n        for inputs, targets in loader:\\n            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\\n            outputs = model(inputs)\\n            if isinstance(loss_fn, nn.MSELoss):\\n                targets_onehot = F.one_hot(targets, num_classes=10).float()\\n                loss = loss_fn(outputs, targets_onehot)\\n            else:\\n                loss = loss_fn(outputs, targets)\\n            running_loss += loss.item() * inputs.size(0)\\n            _, preds = outputs.max(1)\\n            correct += preds.eq(targets).sum().item()\\n            total += targets.size(0)\\n\\n    epoch_loss = running_loss / total\\n    acc = 100.0 * correct / total\\n    writer.add_scalar(\"val/loss\", epoch_loss, epoch)\\n    writer.add_scalar(\"val/acc\", acc, epoch)\\n    print(f\"Validation\\xa0Loss: {epoch_loss:.4f}  Acc: {acc:.2f}%\")\\n    return epoch_loss, acc\\n\\n\\n# -------------------------------------------------\\n# Main entry\\n# -------------------------------------------------\\n\\ndef main(argv: list[str] | None = None):\\n    args = parse_args(argv)\\n    set_seed(args.seed, args.deterministic)\\n\\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n    train_loader, val_loader = get_dataloaders(args)\\n\\n    model = MLP(in_dim=3*32*32, num_classes=10,\\n                hidden_dims=args.hidden_dims, dropout=args.dropout).to(device)\\n\\n    loss_fn = choose_loss_fn(args)\\n    optimizer = choose_optimizer(model.parameters(), args)\\n    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\\n                                                     milestones=args.milestones,\\n                                                     gamma=args.gamma)\\n\\n    logdir = Path(args.log_dir) / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\\n    writer = SummaryWriter(logdir)\\n    writer.add_text(\"hyperparameters\", str(vars(args)))\\n\\n    best_acc = 0.0\\n    for epoch in range(1, args.epochs + 1):\\n        print(f\"========== Epoch {epoch}/{args.epochs} ==========\")\\n        train_one_epoch(model, train_loader, loss_fn, optimizer,\\n                        device, epoch, writer, args.print_freq)\\n        _, val_acc = evaluate(model, val_loader, loss_fn, device, epoch, writer)\\n        scheduler.step()\\n\\n        writer.add_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], epoch)\\n\\n        if val_acc > best_acc:\\n            best_acc = val_acc\\n            ckpt_path = logdir / \"best.pt\"\\n            torch.save({\\n                \"model_state\": model.state_dict(),\\n                \"optimizer_state\": optimizer.state_dict(),\\n                \"args\": vars(args),\\n                \"epoch\": epoch,\\n                \"best_acc\": best_acc,\\n            }, ckpt_path)\\n            print(f\"[INFO] Saved checkpoint → {ckpt_path} (acc={best_acc:.2f}%)\")\\n\\n    print(f\"Training complete. Best validation accuracy: {best_acc:.2f}%\")\\n    writer.close()\\n\\n\\nif __name__ == \"__main__\":\\n    # In notebooks: supply sys.argv[1:] to preserve cell‑line flags but ignore Jupyter\\'s internal -f\\n    main(sys.argv[1:])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\"\"\"# cifar10_mlp.py – Rev‑3 (final bracket fix, fully syntactically valid)\n",
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "import argparse\n",
        "import random\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Argument parsing (swallows unknown Jupyter flags)\n",
        "# -------------------------------------------------\n",
        "\n",
        "def parse_args(argv: list[str] | None = None) -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser(\"CIFAR‑10 MLP trainer\", add_help=True)\n",
        "\n",
        "    # General\n",
        "    p.add_argument(\"--epochs\", type=int, default=20)\n",
        "    p.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    p.add_argument(\"--subset\", type=int, default=None,\n",
        "                   help=\"Use only N training samples for quick iteration\")\n",
        "    p.add_argument(\"--data-root\", type=str, default=\"./data\")\n",
        "    p.add_argument(\"--workers\", type=int, default=4)\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "\n",
        "    # Model\n",
        "    p.add_argument(\"--hidden-dims\", type=int, nargs=\"*\", default=[1024, 512, 256])\n",
        "    p.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "\n",
        "    # Optimizer / LR\n",
        "    p.add_argument(\"--optimizer\", choices=[\"sgd\", \"adam\", \"adagrad\"], default=\"sgd\")\n",
        "    p.add_argument(\"--lr\", type=float, default=0.1)\n",
        "    p.add_argument(\"--momentum\", type=float, default=0.9)\n",
        "    p.add_argument(\"--weight-decay\", type=float, default=1e-4)\n",
        "    p.add_argument(\"--milestones\", type=int, nargs=\"*\", default=[60, 120, 160])\n",
        "    p.add_argument(\"--gamma\", type=float, default=0.2)\n",
        "\n",
        "    # Loss\n",
        "    p.add_argument(\"--loss\", choices=[\"cross_entropy\", \"label_smooth\", \"mse\"],\n",
        "                   default=\"cross_entropy\")\n",
        "    p.add_argument(\"--label-smoothing\", type=float, default=0.1,\n",
        "                   help=\"Only used with label_smooth loss option\")\n",
        "\n",
        "    # Misc\n",
        "    p.add_argument(\"--log-dir\", type=str, default=\"runs\")\n",
        "    p.add_argument(\"--print-freq\", type=int, default=100)\n",
        "    p.add_argument(\"--deterministic\", action=\"store_true\",\n",
        "                   help=\"Force deterministic cuDNN (slower)\")\n",
        "\n",
        "    args, unknown = p.parse_known_args(argv)\n",
        "    if unknown:\n",
        "        print(f\"[WARNING] Ignoring unknown arguments: {unknown}\")\n",
        "    return args\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Model definition\n",
        "# -------------------------------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, num_classes: int, hidden_dims: list[int], dropout: float):\n",
        "        super().__init__()\n",
        "        layers: list[nn.Module] = []\n",
        "        last_dim = in_dim\n",
        "        for h in hidden_dims:\n",
        "            layers.append(nn.Linear(last_dim, h))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            last_dim = h\n",
        "        layers.append(nn.Linear(last_dim, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Helper utilities\n",
        "# -------------------------------------------------\n",
        "\n",
        "def set_seed(seed: int, deterministic: bool = False):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    cudnn.benchmark = not deterministic\n",
        "    cudnn.deterministic = deterministic\n",
        "\n",
        "\n",
        "def get_dataloaders(args: argparse.Namespace):\n",
        "    transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root=args.data_root, train=True, download=True,\n",
        "                                           transform=transform)\n",
        "    testset = torchvision.datasets.CIFAR10(root=args.data_root, train=False, download=True,\n",
        "                                          transform=transform)\n",
        "\n",
        "    if args.subset:\n",
        "        subset_idx = torch.randperm(len(trainset))[:args.subset]\n",
        "        trainset = torch.utils.data.Subset(trainset, subset_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,\n",
        "                                              shuffle=True, num_workers=args.workers,\n",
        "                                              pin_memory=True)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size,\n",
        "                                             shuffle=False, num_workers=args.workers,\n",
        "                                             pin_memory=True)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def choose_loss_fn(args: argparse.Namespace):\n",
        "    if args.loss == \"cross_entropy\":\n",
        "        return nn.CrossEntropyLoss()\n",
        "    if args.loss == \"label_smooth\":\n",
        "        return nn.CrossEntropyLoss(label_smoothing=args.label_smoothing)\n",
        "    if args.loss == \"mse\":\n",
        "        return nn.MSELoss()\n",
        "    raise ValueError(args.loss)\n",
        "\n",
        "\n",
        "def choose_optimizer(params, args: argparse.Namespace):\n",
        "    if args.optimizer == \"sgd\":\n",
        "        return torch.optim.SGD(params, lr=args.lr, momentum=args.momentum, nesterov=True,\n",
        "                               weight_decay=args.weight_decay)\n",
        "    if args.optimizer == \"adam\":\n",
        "        return torch.optim.Adam(params, lr=args.lr, weight_decay=args.weight_decay)\n",
        "    if args.optimizer == \"adagrad\":\n",
        "        return torch.optim.Adagrad(params, lr=args.lr, weight_decay=args.weight_decay)\n",
        "    raise ValueError(args.optimizer)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Training / Evaluation loops\n",
        "# -------------------------------------------------\n",
        "\n",
        "def train_one_epoch(model, loader, loss_fn, optimizer, device, epoch, writer, print_freq):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for i, (inputs, targets) in enumerate(loader):\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        if isinstance(loss_fn, nn.MSELoss):\n",
        "            targets_onehot = F.one_hot(targets, num_classes=10).float()\n",
        "            loss = loss_fn(outputs, targets_onehot)\n",
        "        else:\n",
        "            loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += preds.eq(targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        if (i + 1) % print_freq == 0:\n",
        "            print(f\"Epoch [{epoch}] Iter [{i+1}/{len(loader)}]  Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    acc = 100.0 * correct / total\n",
        "    writer.add_scalar(\"train/loss\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"train/acc\", acc, epoch)\n",
        "    return epoch_loss, acc\n",
        "\n",
        "\n",
        "def evaluate(model, loader, loss_fn, device, epoch, writer):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(loss_fn, nn.MSELoss):\n",
        "                targets_onehot = F.one_hot(targets, num_classes=10).float()\n",
        "                loss = loss_fn(outputs, targets_onehot)\n",
        "            else:\n",
        "                loss = loss_fn(outputs, targets)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += preds.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    acc = 100.0 * correct / total\n",
        "    writer.add_scalar(\"val/loss\", epoch_loss, epoch)\n",
        "    writer.add_scalar(\"val/acc\", acc, epoch)\n",
        "    print(f\"Validation Loss: {epoch_loss:.4f}  Acc: {acc:.2f}%\")\n",
        "    return epoch_loss, acc\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# Main entry\n",
        "# -------------------------------------------------\n",
        "\n",
        "def main(argv: list[str] | None = None):\n",
        "    args = parse_args(argv)\n",
        "    set_seed(args.seed, args.deterministic)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    train_loader, val_loader = get_dataloaders(args)\n",
        "\n",
        "    model = MLP(in_dim=3*32*32, num_classes=10,\n",
        "                hidden_dims=args.hidden_dims, dropout=args.dropout).to(device)\n",
        "\n",
        "    loss_fn = choose_loss_fn(args)\n",
        "    optimizer = choose_optimizer(model.parameters(), args)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                     milestones=args.milestones,\n",
        "                                                     gamma=args.gamma)\n",
        "\n",
        "    logdir = Path(args.log_dir) / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    writer = SummaryWriter(logdir)\n",
        "    writer.add_text(\"hyperparameters\", str(vars(args)))\n",
        "\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        print(f\"========== Epoch {epoch}/{args.epochs} ==========\")\n",
        "        train_one_epoch(model, train_loader, loss_fn, optimizer,\n",
        "                        device, epoch, writer, args.print_freq)\n",
        "        _, val_acc = evaluate(model, val_loader, loss_fn, device, epoch, writer)\n",
        "        scheduler.step()\n",
        "\n",
        "        writer.add_scalar(\"lr\", optimizer.param_groups[0][\"lr\"], epoch)\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            ckpt_path = logdir / \"best.pt\"\n",
        "            torch.save({\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"optimizer_state\": optimizer.state_dict(),\n",
        "                \"args\": vars(args),\n",
        "                \"epoch\": epoch,\n",
        "                \"best_acc\": best_acc,\n",
        "            }, ckpt_path)\n",
        "            print(f\"[INFO] Saved checkpoint → {ckpt_path} (acc={best_acc:.2f}%)\")\n",
        "\n",
        "    print(f\"Training complete. Best validation accuracy: {best_acc:.2f}%\")\n",
        "    writer.close()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # In notebooks: supply sys.argv[1:] to preserve cell‑line flags but ignore Jupyter's internal -f\n",
        "    main(sys.argv[1:])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile cifar10_mlp.py\n",
        "\n",
        "# cifar10_mlp.py – Rev‑6 (grid search & Colab‑ready)\n",
        "\"\"\"\n",
        "CIFAR‑10 Multilayer‑Perceptron mit optionalem Grid‑Search (Loss × Optimizer)\n",
        "\n",
        "Features (E1–E4)\n",
        "----------------\n",
        "* TensorBoard‑Logging pro Trial: `runs/<timestamp>/<trial>`\n",
        "* Zwei Loss‑Funktionen: Cross‑Entropy, Label‑Smoothed CE (ε = 0.1)\n",
        "* Zwei Optimizer: SGD + Nesterov & Adam\n",
        "* Multi‑Step‑LR‑Schedule (Milestones 60/120/160, γ=0.2)\n",
        "\n",
        "Beispiele\n",
        "~~~~~~~~~\n",
        "# Einzel‑Run\n",
        "!python cifar10_mlp.py --epochs 10 --subset 10000 --hidden-dims 512 256\n",
        "\n",
        "# Grid‑Search (4 Runs × 2 Epochen × 10k Samples)\n",
        "!python cifar10_mlp.py --tune --epochs 2 --subset 10000 --batch-size 256 \\\n",
        "                       --hidden-dims 512 256 --log-dir runs_grid\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import copy\n",
        "import itertools\n",
        "import random\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Argument Parsing\n",
        "# ---------------------------------------------------------------------\n",
        "def parse_args(argv: List[str] | None = None) -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser(\"CIFAR‑10 MLP trainer (+grid search)\")\n",
        "\n",
        "    # Allgemein\n",
        "    p.add_argument(\"--epochs\", type=int, default=20)\n",
        "    p.add_argument(\"--batch-size\", type=int, default=128)\n",
        "    p.add_argument(\"--subset\", type=int, default=None,\n",
        "                   help=\"Verwende nur N Trainings‑Samples (schnelleres Prototyping)\")\n",
        "    p.add_argument(\"--data-root\", type=str, default=\"./data\")\n",
        "    p.add_argument(\"--workers\", type=int, default=2)\n",
        "    p.add_argument(\"--seed\", type=int, default=42)\n",
        "\n",
        "    # Modell\n",
        "    p.add_argument(\"--hidden-dims\", type=int, nargs=\"*\", default=[1024, 512, 256])\n",
        "    p.add_argument(\"--dropout\", type=float, default=0.0)\n",
        "\n",
        "    # Optimizer / LR\n",
        "    p.add_argument(\"--optimizer\", choices=[\"sgd\", \"adam\", \"adagrad\"], default=\"sgd\")\n",
        "    p.add_argument(\"--lr\", type=float, default=0.1)\n",
        "    p.add_argument(\"--momentum\", type=float, default=0.9)\n",
        "    p.add_argument(\"--weight-decay\", type=float, default=1e-4)\n",
        "    p.add_argument(\"--milestones\", type=int, nargs=\"*\", default=[60, 120, 160])\n",
        "    p.add_argument(\"--gamma\", type=float, default=0.2)\n",
        "\n",
        "    # Loss\n",
        "    p.add_argument(\"--loss\", choices=[\"cross_entropy\", \"label_smooth\"],\n",
        "                   default=\"cross_entropy\")\n",
        "    p.add_argument(\"--label-smoothing\", type=float, default=0.1)\n",
        "\n",
        "    # Diverses\n",
        "    p.add_argument(\"--log-dir\", type=str, default=\"runs\")\n",
        "    p.add_argument(\"--print-freq\", type=int, default=100)\n",
        "    p.add_argument(\"--deterministic\", action=\"store_true\")\n",
        "    p.add_argument(\"--tune\", action=\"store_true\",\n",
        "                   help=\"Führe vordefiniertes Loss/Optimizer‑Grid (4 Trials) aus\")\n",
        "\n",
        "    args, unknown = p.parse_known_args(argv)\n",
        "    if unknown:\n",
        "        print(f\"[WARNING] Ignoring unknown arguments: {unknown}\")\n",
        "    return args\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Modell‑Definition\n",
        "# ---------------------------------------------------------------------\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int, num_classes: int,\n",
        "                 hidden_dims: List[int], dropout: float):\n",
        "        super().__init__()\n",
        "        layers: List[nn.Module] = []\n",
        "        last = in_dim\n",
        "        for h in hidden_dims:\n",
        "            layers += [\n",
        "                nn.Linear(last, h, bias=False),\n",
        "                nn.BatchNorm1d(h),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Dropout(p=dropout)]\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            last = h\n",
        "        layers.append(nn.Linear(last, num_classes))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:           # type: ignore[override]\n",
        "        return self.net(torch.flatten(x, 1))\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Hilfsfunktionen\n",
        "# ---------------------------------------------------------------------\n",
        "def set_seed(seed: int, deterministic: bool = False):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    cudnn.benchmark = not deterministic\n",
        "    cudnn.deterministic = deterministic\n",
        "\n",
        "def get_dataloaders(a: argparse.Namespace):\n",
        "    tf = T.Compose([T.ToTensor(),\n",
        "                    T.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                                (0.247, 0.243, 0.261))])\n",
        "    trainset = torchvision.datasets.CIFAR10(a.data_root, train=True,\n",
        "                                            download=True, transform=tf)\n",
        "    testset  = torchvision.datasets.CIFAR10(a.data_root, train=False,\n",
        "                                            download=True, transform=tf)\n",
        "    if a.subset:\n",
        "        trainset = torch.utils.data.Subset(\n",
        "            trainset, torch.randperm(len(trainset))[:a.subset])\n",
        "\n",
        "    tr_loader  = torch.utils.data.DataLoader(trainset, batch_size=a.batch_size,\n",
        "                                             shuffle=True,  num_workers=a.workers,\n",
        "                                             pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(testset,  batch_size=a.batch_size,\n",
        "                                             shuffle=False, num_workers=a.workers,\n",
        "                                             pin_memory=True)\n",
        "    return tr_loader, val_loader\n",
        "\n",
        "def choose_loss_fn(a: argparse.Namespace):\n",
        "    return (nn.CrossEntropyLoss(label_smoothing=a.label_smoothing)\n",
        "            if a.loss == \"label_smooth\"\n",
        "            else nn.CrossEntropyLoss())\n",
        "\n",
        "def choose_optimizer(params, a: argparse.Namespace):\n",
        "    if a.optimizer == \"sgd\":\n",
        "        return torch.optim.SGD(params, lr=a.lr, momentum=a.momentum,\n",
        "                               nesterov=True, weight_decay=a.weight_decay)\n",
        "    if a.optimizer == \"adam\":\n",
        "        return torch.optim.Adam(params, lr=a.lr, weight_decay=a.weight_decay)\n",
        "    if a.optimizer == \"adagrad\":\n",
        "        return torch.optim.Adagrad(params, lr=a.lr, weight_decay=a.weight_decay)\n",
        "    raise ValueError(a.optimizer)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Training / Validierung\n",
        "# ---------------------------------------------------------------------\n",
        "def train_one_epoch(model, loader, loss_fn, optim, device, epoch, writer, pf):\n",
        "    model.train()\n",
        "    correct = total = 0\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        optim.zero_grad(set_to_none=True)\n",
        "        out  = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        _, pred = out.max(1)\n",
        "        correct += pred.eq(y).sum().item()\n",
        "        total   += y.size(0)\n",
        "        if (i + 1) % pf == 0:\n",
        "            print(f\"Ep{epoch}  Iter {i+1}/{len(loader)}  Loss {loss.item():.4f}\")\n",
        "\n",
        "    acc = 100.0 * correct / total\n",
        "    writer.add_scalar(\"train/acc\", acc, epoch)\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, loader, device, epoch, writer):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "            _, pred = model(x).max(1)\n",
        "            correct += pred.eq(y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    acc = 100.0 * correct / total\n",
        "    writer.add_scalar(\"val/acc\", acc, epoch)\n",
        "    return acc\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Einzel‑Experiment\n",
        "# ---------------------------------------------------------------------\n",
        "def run_experiment(a: argparse.Namespace, trial: str) -> float:\n",
        "    set_seed(a.seed, a.deterministic)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tr_loader, val_loader = get_dataloaders(a)\n",
        "\n",
        "    model = MLP(3*32*32, 10, a.hidden_dims, a.dropout).to(device)\n",
        "    loss_fn = choose_loss_fn(a)\n",
        "    optim   = choose_optimizer(model.parameters(), a)\n",
        "    sched   = torch.optim.lr_scheduler.MultiStepLR(\n",
        "                  optim, milestones=a.milestones, gamma=a.gamma)\n",
        "\n",
        "    writer = SummaryWriter(Path(a.log_dir) / trial)\n",
        "    writer.add_text(\"hyperparameters\", str(vars(a)))\n",
        "\n",
        "    best = 0.0\n",
        "    for ep in range(1, a.epochs + 1):\n",
        "        train_one_epoch(model, tr_loader, loss_fn, optim, device, ep,\n",
        "                        writer, a.print_freq)\n",
        "        val_acc = evaluate(model, val_loader, device, ep, writer)\n",
        "        best = max(best, val_acc)\n",
        "        sched.step()\n",
        "        writer.add_scalar(\"lr\", optim.param_groups[0][\"lr\"], ep)\n",
        "    writer.close()\n",
        "    return best\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Main (Single‑Run oder Grid‑Search)\n",
        "# ---------------------------------------------------------------------\n",
        "def main(argv: List[str] | None = None):\n",
        "    base = parse_args(argv)\n",
        "\n",
        "    if not base.tune:\n",
        "        tag  = datetime.now().strftime(\"%Y%m%d-%H%M%S_single\")\n",
        "        best = run_experiment(base, tag)\n",
        "        print(f\"Single run finished – Best Val Acc: {best:.2f}%\")\n",
        "        return\n",
        "\n",
        "    # Grid: loss × optimizer\n",
        "    losses = [\"cross_entropy\", \"label_smooth\"]\n",
        "    opts   = [\"sgd\", \"adam\"]\n",
        "    grid   = list(itertools.product(losses, opts))\n",
        "\n",
        "    leaderboard: List[Tuple[str, str, str, float]] = []\n",
        "    for i, (loss, opt) in enumerate(grid, 1):\n",
        "        exp = copy.deepcopy(base)\n",
        "        exp.loss      = loss\n",
        "        exp.optimizer = opt\n",
        "        exp.lr        = 0.1 if opt == \"sgd\" else 0.001\n",
        "        tag = datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"_L-{loss}_O-{opt}\"\n",
        "        print(f\"\\n==== Trial {i}/{len(grid)} → {tag} ====\")\n",
        "        best = run_experiment(exp, tag)\n",
        "        leaderboard.append((tag, loss, opt, best))\n",
        "\n",
        "    leaderboard.sort(key=lambda t: t[-1], reverse=True)\n",
        "    print(\"\\n==== Leaderboard ====\")\n",
        "    for rk, (tag, loss, opt, acc) in enumerate(leaderboard, 1):\n",
        "        print(f\"{rk:2d}. {acc:5.2f}%  |  opt={opt:<4s}  loss={loss:<14s}  id={tag}\")\n",
        "\n",
        "if __name__ == \"__main__\":        # erlaubt  !python cifar10_mlp.py ...\n",
        "    main(sys.argv[1:])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7QmLZT-lO3Hz",
        "outputId": "211e5ce5-5b66-457b-dd10-160c4ea441e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cifar10_mlp.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Grid‑Search starten (Beispiel)\n",
        "!python cifar10_mlp.py --tune --epochs 40 --subset 10000 --batch-size 256 \\\n",
        "                       --hidden-dims 1024 512 256 --log-dir runs_grid"
      ],
      "metadata": {
        "id": "FHObQ1t_Uzsw",
        "outputId": "9cffba32-9d8f-4a56-f096-38997e05187f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-22 08:49:45.419460: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745311785.439325     364 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745311785.445408     364 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 08:49:45.465407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "==== Trial 1/4 → 20250422-084948_L-cross_entropy_O-sgd ====\n",
            "100% 170M/170M [00:02<00:00, 57.8MB/s]\n",
            "\n",
            "==== Trial 2/4 → 20250422-085300_L-cross_entropy_O-adam ====\n",
            "\n",
            "==== Trial 3/4 → 20250422-085605_L-label_smooth_O-sgd ====\n",
            "\n",
            "==== Trial 4/4 → 20250422-085910_L-label_smooth_O-adam ====\n",
            "\n",
            "==== Leaderboard ====\n",
            " 1. 47.23%  |  opt=adam  loss=cross_entropy   id=20250422-085300_L-cross_entropy_O-adam\n",
            " 2. 47.02%  |  opt=adam  loss=label_smooth    id=20250422-085910_L-label_smooth_O-adam\n",
            " 3. 45.88%  |  opt=sgd   loss=label_smooth    id=20250422-085605_L-label_smooth_O-sgd\n",
            " 4. 45.45%  |  opt=sgd   loss=cross_entropy   id=20250422-084948_L-cross_entropy_O-sgd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python cifar10_mlp.py \\\n",
        "        --epochs 20 \\\n",
        "        --batch-size 256 \\\n",
        "        --hidden-dims 1024 512 256 \\\n",
        "        --optimizer adam \\\n",
        "        --loss cross_entropy \\\n",
        "        --lr 0.001 \\\n",
        "        --workers 2 \\\n",
        "        --log-dir runs_single\n"
      ],
      "metadata": {
        "id": "Y1_a_CAkW8uG",
        "outputId": "37d13f96-505e-4c06-e0e0-2b412a9115a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-22 09:05:50.440369: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745312750.460526    7689 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745312750.466391    7689 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-22 09:05:50.487177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Ep1  Iter 100/196  Loss 1.4289\n",
            "Ep2  Iter 100/196  Loss 1.3719\n",
            "Ep3  Iter 100/196  Loss 1.0810\n",
            "Ep4  Iter 100/196  Loss 1.1549\n",
            "Ep5  Iter 100/196  Loss 1.1229\n",
            "Ep6  Iter 100/196  Loss 0.9974\n",
            "Ep7  Iter 100/196  Loss 1.0773\n",
            "Ep8  Iter 100/196  Loss 0.7941\n",
            "Ep9  Iter 100/196  Loss 0.7732\n",
            "Ep10  Iter 100/196  Loss 0.7461\n",
            "Ep11  Iter 100/196  Loss 0.7119\n",
            "Ep12  Iter 100/196  Loss 0.6875\n",
            "Ep13  Iter 100/196  Loss 0.6562\n",
            "Ep14  Iter 100/196  Loss 0.5545\n",
            "Ep15  Iter 100/196  Loss 0.5147\n",
            "Ep16  Iter 100/196  Loss 0.5251\n",
            "Ep17  Iter 100/196  Loss 0.4316\n",
            "Ep18  Iter 100/196  Loss 0.3774\n",
            "Ep19  Iter 100/196  Loss 0.2575\n",
            "Ep20  Iter 100/196  Loss 0.2884\n",
            "Single run finished – Best Val Acc: 56.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvdPWUfuva6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment_CIFAR10_MLP_optimization.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}