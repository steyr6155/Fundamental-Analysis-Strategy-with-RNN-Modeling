{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mReKErnKCSXj"
      },
      "source": [
        "# Assigment: PyTorch Model Zoo\n",
        "\n",
        "The [PyTorch \"Model Zoo\"](https://pytorch.org/vision/stable/models.html) provides a large number of pre-trained CNN models and vision [data sets](https://pytorch.org/vision/stable/datasets.html)..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KmAdRglbCSXr"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sUKIJQ9GCSXt",
        "outputId": "0b975cd6-b7b4-477b-ec78-e215b02d2e31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:12<00:00, 13.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "#transform input data (image) to tensor\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "#set batch size\n",
        "batch_size = 4\n",
        "\n",
        "#First dataloader\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMFM5DeGCSXv"
      },
      "source": [
        "## Assignment 1:\n",
        "Load a \"*ResNet18*\" from the torchvision model zoo and train it for 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#Load ResNet‑18 vom Zoo ohne vortraining! (DEFAULT)\n",
        "model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIPVBOmmuGLN",
        "outputId": "a77809fb-5fd8-4577-b8a1-b96f7fe25fc2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 173MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss function + optimiser\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#SGD\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.00005)\n"
      ],
      "metadata": {
        "id": "ulMrQjWFuJyG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#______________________________\n",
        "#Training (10 epochs)\n",
        "#______________________________\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(trainset)\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "    print(f\"Epoche [{epoch+1:02d}/{num_epochs}]  \"\n",
        "          f\"Train‑loss: {epoch_loss:.4f}  Train‑acc: {epoch_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjoNOk2juNIM",
        "outputId": "e4b4115b-6195-4ac8-931d-d44d4936602a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoche [01/10]  Train‑loss: 1.9283  Train‑acc: 35.03%\n",
            "Epoche [02/10]  Train‑loss: 1.4998  Train‑acc: 48.65%\n",
            "Epoche [03/10]  Train‑loss: 1.3443  Train‑acc: 55.01%\n",
            "Epoche [04/10]  Train‑loss: 1.2110  Train‑acc: 59.87%\n",
            "Epoche [05/10]  Train‑loss: 1.2049  Train‑acc: 60.22%\n",
            "Epoche [06/10]  Train‑loss: 1.1367  Train‑acc: 62.33%\n",
            "Epoche [07/10]  Train‑loss: 1.0441  Train‑acc: 65.38%\n",
            "Epoche [08/10]  Train‑loss: 1.0583  Train‑acc: 65.03%\n",
            "Epoche [09/10]  Train‑loss: 1.0375  Train‑acc: 65.95%\n",
            "Epoche [10/10]  Train‑loss: 0.9830  Train‑acc: 67.89%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JWCWLdvVCSXw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "784da195-45d1-4e78-f582-b722585df055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test‑loss: 1.7221  Test‑acc: 63.81%\n"
          ]
        }
      ],
      "source": [
        "#______________________________________________________\n",
        "# Eval\n",
        "#_______________________________________________________\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in testloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        test_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "test_loss /= len(testset)\n",
        "test_acc = 100.0 * correct / total\n",
        "print(f\"\\nTest‑loss: {test_loss:.4f}  Test‑acc: {test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjhXvjZSCSXw"
      },
      "source": [
        "## Assigment 2:\n",
        "Load a **pre-trained** (on ImageNet) \"*ResNet18*\" from the torchvision model zoo and *fine-tune* it for ten epochs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform‑Pipeline – auf 224×224 skalieren, weil vortrainiert, erwarten cnn filter  224px werte! wichtig, sonst sind ergebnisse nicht räpresentativ!\n",
        "transform_ft = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "#DataLoader neu instanziieren\n",
        "trainset_ft = torchvision.datasets.CIFAR10(\"./data\", train=True,  download=False, transform=transform_ft)\n",
        "testset_ft  = torchvision.datasets.CIFAR10(\"./data\", train=False, download=False, transform=transform_ft)\n",
        "\n",
        "trainloader_ft = torch.utils.data.DataLoader(trainset_ft, batch_size=batch_size,\n",
        "                                             shuffle=True,  num_workers=2, pin_memory=True)\n",
        "testloader_ft  = torch.utils.data.DataLoader(testset_ft,  batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "qdr_jv2vBGsN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Pre‑trained ResNet‑18 -- nicht die default weights wie ass.1\n",
        "model_ft = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model_ft.fc = torch.nn.Linear(model_ft.fc.in_features, 10)  # CIFAR‑10 --> 10 Klassen\n",
        "model_ft = model_ft.to(device)\n"
      ],
      "metadata": {
        "id": "GdkkpkYZBLFM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Backbone einfrieren, nur den neuen Kopf trainieren\n",
        "for p in model_ft.parameters():\n",
        "    p.requires_grad = False\n",
        "for p in model_ft.fc.parameters():\n",
        "    p.requires_grad = True\n"
      ],
      "metadata": {
        "id": "Aszl6oF4BSL8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#criterion und Optimizer --> CE, da nicht ordinal und std lr und wd werte, diesmal Adam gewählt\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_ft.fc.parameters(), lr=3e-4, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "qa7h_vJMBTus"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WcuX8tYdCSXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbb2035-6d0f-464b-a2d8-4b82ec30c6f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/10] train_loss=1.1658  train_acc=61.27%\n",
            "[02/10] train_loss=0.9938  train_acc=66.82%\n",
            "[03/10] train_loss=0.9804  train_acc=67.22%\n",
            "[04/10] train_loss=0.9711  train_acc=67.62%\n",
            "[05/10] train_loss=0.9635  train_acc=67.67%\n",
            "[06/10] train_loss=0.9534  train_acc=68.14%\n",
            "[07/10] train_loss=0.9580  train_acc=68.05%\n",
            "[08/10] train_loss=0.9536  train_acc=68.18%\n",
            "[09/10] train_loss=0.9475  train_acc=68.58%\n",
            "[10/10] train_loss=0.9447  train_acc=68.48%\n"
          ]
        }
      ],
      "source": [
        "#______________________________\n",
        "#Training (10 epochs)\n",
        "#______________________________\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model_ft.train()\n",
        "    tot_loss, tot_correct, tot_samples = 0.0, 0, 0\n",
        "\n",
        "    for x, y in trainloader_ft:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model_ft(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tot_loss += loss.item() * x.size(0)\n",
        "        tot_correct += logits.argmax(1).eq(y).sum().item()\n",
        "        tot_samples += y.size(0)\n",
        "\n",
        "    print(f\"[{epoch+1:02d}/{epochs}] \"\n",
        "          f\"train_loss={tot_loss/tot_samples:.4f}  \"\n",
        "          f\"train_acc={100*tot_correct/tot_samples:.2f}%\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#______________________________________________________\n",
        "# Eval\n",
        "#_______________________________________________________\n",
        "model_ft.eval()\n",
        "test_loss, test_correct, test_total = 0.0, 0, 0\n",
        "with torch.no_grad():\n",
        "    for x, y in testloader_ft:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model_ft(x)\n",
        "        test_loss += criterion(logits, y).item() * x.size(0)\n",
        "        test_correct += logits.argmax(1).eq(y).sum().item()\n",
        "        test_total += y.size(0)\n",
        "\n",
        "print(f\"\\nTest_loss={test_loss/test_total:.4f}  Test_acc={100*test_correct/test_total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLe08ZRG_Zfe",
        "outputId": "f255a296-271b-4543-c416-e3ebc7c13ed2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test_loss=0.6421  Test_acc=78.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wie erwartet ist das Ergebnis aus ass2 mit dem pretrained gewcihten etwas besser (ich persönlich dachte es wäre deutlich besser)\n",
        "Ass1 zeigt allerdings einen leichten overfit und scheint nicht optimal zu generalisieren\n",
        "\n",
        "Ass2 scheint zumindest bei diesem sample sehr gute testergebnisse zu liefern. Leider ist wie unten nochmals erwähnt der vollständige test ohne backbone nicht möglich (wollte ich interessenshalber mal testen) um weitere rückschlüsse zu ziehen\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Mögliche verbesserungen:\n",
        "besseres lr rrate scheduling (cosin cyclic learning rate)\n",
        "data augmentation (verschiedene bildausschnitte oder bilder drehen usw)"
      ],
      "metadata": {
        "id": "qc3qrzApBV0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "________________________________________________________________________________\n"
      ],
      "metadata": {
        "id": "hmj16-IEA7-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier ohne backbone und fulltest:\n"
      ],
      "metadata": {
        "id": "sRcOgaw5AttX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform‑Pipeline – auf 224×224 skalieren, weil vortrainiert, erwarten cnn filter  224px werte! wichtig, sonst sind ergebnisse nicht räpresentativ!\n",
        "transform_ft = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "#DataLoader neu instanziieren\n",
        "trainset_ft = torchvision.datasets.CIFAR10(\"./data\", train=True,  download=False, transform=transform_ft)\n",
        "testset_ft  = torchvision.datasets.CIFAR10(\"./data\", train=False, download=False, transform=transform_ft)\n",
        "\n",
        "trainloader_ft = torch.utils.data.DataLoader(trainset_ft, batch_size=batch_size,\n",
        "                                             shuffle=True,  num_workers=2, pin_memory=True)\n",
        "testloader_ft  = torch.utils.data.DataLoader(testset_ft,  batch_size=batch_size,\n",
        "                                             shuffle=False, num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "Cf59vIJBAw8-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Pre‑trained ResNet‑18 -- nicht die default weights wie ass.1\n",
        "model_ft = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "model_ft.fc = torch.nn.Linear(model_ft.fc.in_features, 10)  # CIFAR‑10 --> 10 Klassen\n",
        "model_ft = model_ft.to(device)\n"
      ],
      "metadata": {
        "id": "w48hlnw-AzXt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#criterion und Optimizer --> CE, da nicht ordinal und std lr und wd werte, diesmal Adam gewählt\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_ft.fc.parameters(), lr=3e-4, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "jHcMY4JJA1sV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#______________________________\n",
        "#Training (10 epochs)\n",
        "#______________________________\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model_ft.train()\n",
        "    tot_loss, tot_correct, tot_samples = 0.0, 0, 0\n",
        "\n",
        "    for x, y in trainloader_ft:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model_ft(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tot_loss += loss.item() * x.size(0)\n",
        "        tot_correct += logits.argmax(1).eq(y).sum().item()\n",
        "        tot_samples += y.size(0)\n",
        "\n",
        "    print(f\"[{epoch+1:02d}/{epochs}] \"\n",
        "          f\"train_loss={tot_loss/tot_samples:.4f}  \"\n",
        "          f\"train_acc={100*tot_correct/tot_samples:.2f}%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe6aGSfLA2ft",
        "outputId": "f061049a-77f1-4810-98dc-bc6d8e003994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/10] train_loss=1.1699  train_acc=61.06%\n",
            "[02/10] train_loss=1.0016  train_acc=66.29%\n",
            "[03/10] train_loss=0.9736  train_acc=67.50%\n",
            "[04/10] train_loss=0.9722  train_acc=67.77%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#______________________________________________________\n",
        "# Eval\n",
        "#_______________________________________________________\n",
        "model_ft.eval()\n",
        "test_loss, test_correct, test_total = 0.0, 0, 0\n",
        "with torch.no_grad():\n",
        "    for x, y in testloader_ft:\n",
        "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
        "        logits = model_ft(x)\n",
        "        test_loss += criterion(logits, y).item() * x.size(0)\n",
        "        test_correct += logits.argmax(1).eq(y).sum().item()\n",
        "        test_total += y.size(0)\n",
        "\n",
        "print(f\"\\nTest_loss={test_loss/test_total:.4f}  Test_acc={100*test_correct/test_total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ZW5dbfbtA44Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "leider habe ich die gpu nutzung überschritten, weshalb der letzte test nicht vollständig evaluiert werden kann..."
      ],
      "metadata": {
        "id": "0ML0jfbXLpwG"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Assignment_2_PyTorch_Model_Zoo.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}