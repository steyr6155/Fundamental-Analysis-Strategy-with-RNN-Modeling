{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_CIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Vision Transformers on CIFAR10"
      ],
      "metadata": {
        "id": "9rZPsOU0phDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i0AIosM0AaY2"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "dataset = dset.CIFAR10(root=\"./data\", download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "nc=3\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "is2FpH_lAtJJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the availability of cuda devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "yF-tedlhAx-s"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks:\n",
        "* try to get the best test Accuracy on Cifar10 using a transformer model\n",
        "* pre-trained models allowed\n"
      ],
      "metadata": {
        "id": "KxRD7Myvpogs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "gMhxLnIeBfBe",
        "outputId": "90c128d5-fba6-4f83-c4fc-8c38539185cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vision Transformer on CIFAR-10 kompletter Notebook-Code\n",
        "\n",
        "\n",
        "# 0) Imports & Dynamo-Bypass\n",
        "import torch\n",
        "# Deaktiviere Torch Dynamo, um Circular-Import in Transformers zu verhindern\n",
        "if hasattr(torch, '_dynamo'):\n",
        "    try:\n",
        "        torch._dynamo.disable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTConfig, get_linear_schedule_with_warmup\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification, ViTConfig, get_linear_schedule_with_warmup\n",
        "\n",
        "# 1) Datenvorbereitung\n",
        "# Lade FeatureExtractor für Mean/Std und Patch-Size\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(64),               # CIFAR-ähnliche Auflösung 64x64\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=feature_extractor.image_mean,\n",
        "        std=feature_extractor.image_std\n",
        "    )\n",
        "])\n",
        "\n",
        "dataset = dset.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
        "\n",
        "# DataLoader\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# 2) Device-Check\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 3) Modellinitialisierung mit 64x64-Konfiguration\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "config = ViTConfig.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    image_size=64,\n",
        "    num_labels=10,\n",
        "    id2label={i: cls for i, cls in enumerate(dataset.classes)},\n",
        "    label2id={cls: i for i, cls in enumerate(dataset.classes)}\n",
        ")\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True    # interpoliert Position-Embeddings\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# 4) Optimizer & Scheduler\n",
        "epochs = 5\n",
        "total_steps = len(dataloader) * epochs\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "# 5) Trainingsschleife\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(pixel_values=imgs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(dataloader)\n",
        "    train_acc = correct / total\n",
        "    print(f\"Epoch {epoch}/{epochs} - Loss: {avg_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
        "\n",
        "# 6) Evaluation auf dem Testset\n",
        "# Dataset und DataLoader für Testdaten\n",
        "test_dataset = dset.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "# Modell in Eval-Modus versetzen\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(pixel_values=imgs)\n",
        "        preds = outputs.logits.argmax(dim=-1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "test_acc = correct / total\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# 7) Modell speichern\n",
        "torch.save(model.state_dict(), 'vit64_cifar10.pth')\n",
        "print(\"Training und Evaluation abgeschlossen. Modell gespeichert.\")\n"
      ],
      "metadata": {
        "id": "m_Dbc3R6vENq",
        "outputId": "9bc095e9-2624-4bcb-f1ed-ddfd9a95b6f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized because the shapes did not match:\n",
            "- vit.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 17, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 - Loss: 1.3669, Train Acc: 0.5592\n",
            "Epoch 2/5 - Loss: 0.4419, Train Acc: 0.8682\n",
            "Epoch 3/5 - Loss: 0.2114, Train Acc: 0.9437\n",
            "Epoch 4/5 - Loss: 0.0962, Train Acc: 0.9806\n",
            "Epoch 5/5 - Loss: 0.0505, Train Acc: 0.9938\n",
            "Test Accuracy: 0.8838\n",
            "Training und Evaluation abgeschlossen. Modell gespeichert.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFwJN7LPbybO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}