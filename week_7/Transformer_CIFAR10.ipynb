{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_CIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Vision Transformers on CIFAR10"
      ],
      "metadata": {
        "id": "9rZPsOU0phDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i0AIosM0AaY2"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "dataset = dset.CIFAR10(root=\"./data\", download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "nc=3\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "is2FpH_lAtJJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the availability of cuda devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "yF-tedlhAx-s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks:\n",
        "* try to get the best test Accuracy on Cifar10 using a transformer model\n",
        "* pre-trained models allowed\n"
      ],
      "metadata": {
        "id": "KxRD7Myvpogs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "gMhxLnIeBfBe",
        "outputId": "0aa55ab3-eade-4b3c-d4e6-19a369badfd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "from transformers import (\n",
        "    ViTConfig,\n",
        "    ViTFeatureExtractor,\n",
        "    ViTForImageClassification,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from torch.optim import AdamW"
      ],
      "metadata": {
        "id": "0mlWJW8EEYLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seeden f. reproduzierbarkeit\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "id": "xrHrhFQOEaz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bilder vom pretrained model korrekt skalieren/transformioeren\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\"\n",
        ")\n",
        "\n",
        "vit_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(64),\n",
        "        transforms.CenterCrop(64),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=feature_extractor.image_mean, std=feature_extractor.image_std\n",
        "        ),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "8PEyL4PkEg9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train/test Daten laden\n",
        "train_dataset = dset.CIFAR10(\n",
        "    root=\"./data\", train=True, download=False, transform=vit_transform\n",
        ")\n",
        "test_dataset = dset.CIFAR10(\n",
        "    root=\"./data\", train=False, download=False, transform=vit_transform\n",
        ")\n",
        "\n",
        "#Dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=128, shuffle=True, num_workers=2\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=128, shuffle=False, num_workers=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "01OzOiXrEj-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.backends.cudnn.benchmark = True  # schneller auf GPU\n",
        "\n",
        "config = ViTConfig.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    image_size=64, #img size\n",
        "    num_labels=10, #klassen\n",
        "    id2label={i: c for i, c in enumerate(train_dataset.classes)}, #bidirektionales mapping\n",
        "    label2id={c: i for i, c in enumerate(train_dataset.classes)},\n",
        ")\n",
        "\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True,  #positionembeddings werden interpoliert\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "yOd5yRqIEnXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Optimizer-adamw, scheduler m. warmup\n",
        "epochs = 20\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.05)\n",
        "total_steps = len(train_loader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=int(0.1 * total_steps),\n",
        "    num_training_steps=total_steps,\n",
        ")"
      ],
      "metadata": {
        "id": "wEMGgIzoEqNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train u valid.\n",
        "for ep in range(epochs):\n",
        "    model.train()\n",
        "    ep_loss = 0.0\n",
        "    ep_correct = 0\n",
        "    ep_samples = 0\n",
        "\n",
        "    for imgs, lbls in train_loader:\n",
        "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "\n",
        "        out = model(pixel_values=imgs, labels=lbls)\n",
        "        loss = out.loss\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        ep_loss += loss.item() * imgs.size(0)\n",
        "        ep_correct += (out.logits.argmax(dim=-1) == lbls).sum().item()\n",
        "        ep_samples += lbls.size(0)\n",
        "\n",
        "    train_acc = ep_correct / ep_samples\n",
        "    train_loss = ep_loss / ep_samples\n",
        "\n",
        "    # ─ Validation nach jeder Epoche\n",
        "    model.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, lbls in test_loader:\n",
        "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "            preds = model(pixel_values=imgs).logits.argmax(dim=-1)\n",
        "            val_correct += (preds == lbls).sum().item()\n",
        "            val_total += lbls.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "    print(\n",
        "        f\"[{ep+1:02}/{epochs}]  \"\n",
        "        f\"loss={train_loss:.4f}  \"\n",
        "        f\"train_acc={train_acc:.3f}  \"\n",
        "        f\"val_acc={val_acc:.3f}\"\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "m_Dbc3R6vENq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3c5f7e-55df-4021-e22d-f572c37f1745"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized because the shapes did not match:\n",
            "- vit.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 17, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/20]  loss=2.0393  train_acc=0.320  val_acc=0.621\n",
            "[02/20]  loss=0.8984  train_acc=0.747  val_acc=0.823\n",
            "[03/20]  loss=0.4548  train_acc=0.868  val_acc=0.859\n",
            "[04/20]  loss=0.2553  train_acc=0.932  val_acc=0.868\n",
            "[05/20]  loss=0.1418  train_acc=0.966  val_acc=0.868\n",
            "[06/20]  loss=0.0784  train_acc=0.984  val_acc=0.863\n",
            "[07/20]  loss=0.0477  train_acc=0.992  val_acc=0.875\n",
            "[08/20]  loss=0.0323  train_acc=0.995  val_acc=0.877\n",
            "[09/20]  loss=0.0205  train_acc=0.997  val_acc=0.878\n",
            "[10/20]  loss=0.0153  train_acc=0.998  val_acc=0.879\n",
            "[11/20]  loss=0.0125  train_acc=0.998  val_acc=0.873\n",
            "[12/20]  loss=0.0126  train_acc=0.998  val_acc=0.874\n",
            "[13/20]  loss=0.0092  train_acc=0.998  val_acc=0.874\n",
            "[14/20]  loss=0.0076  train_acc=0.999  val_acc=0.876\n",
            "[15/20]  loss=0.0049  train_acc=0.999  val_acc=0.880\n",
            "[16/20]  loss=0.0033  train_acc=1.000  val_acc=0.881\n",
            "[17/20]  loss=0.0028  train_acc=1.000  val_acc=0.879\n",
            "[18/20]  loss=0.0026  train_acc=1.000  val_acc=0.880\n",
            "[19/20]  loss=0.0024  train_acc=1.000  val_acc=0.880\n",
            "[20/20]  loss=0.0023  train_acc=1.000  val_acc=0.880\n",
            "✓ Training abgeschlossen – Modell gespeichert unter vit_cifar10_pretrained.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFwJN7LPbybO"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}